# HTML Code Generation from Images With Deep Neural Networks

This automated AI system generates the HTML code right directly from uploading a UI/MockUp image to the system. There are two parts here, the encoder part captures and features the images and encode it into inner codes and features and learn the UIs, the decoder part learn the coded bootstrap code provided for that image and learn the stream while learning or training, and then concatenate the both the inner features from encoder and decoder. Then at the last the LSTMs takes those inner features and generates the intermediate bootstrap code as a result, which is then compiled into HTML code through a web compiler. The results achieved here, are higher than that of the expectations even at the evaluation set.

The research work and project is done in my undergrad thesis, you can find it here on [my site](https://taneemishere.github.io/projects/project-one.html).

> Live demo is coming really soon ... !

## The Architecture

![Architecture](https://raw.githubusercontent.com/taneemishere/html-code-generation-from-images-with-deep-neural-networks/main/resources-for-md/My-FYP-Work.png)

Here is this achitecture, the upper Encoder and Decoder are the part of the AutoEncoder<sup>1</sup>, while the rest of the model is part of what we called the Main Model. We give the model, the Images and the MockUps which are speicified as Input Image and Input Code respectively, the MockUps are the DSL Code or the Domain Specific Language. At last the sequential LSTM layers give the generated code, speicified as Output Code here. And this output code is the generated as a dot gui file which is then complied into proper HTML code.

## Project Structure

```
.
|‚îÄ‚îÄ bin                     - contains the model pretrained weights in .h5 and .json 
‚îú‚îÄ‚îÄ compiler                - contains DSL compiler to bootstrap from intermediate code of .gui format
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ assets      
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ classes
‚îú‚îÄ‚îÄ datasets                - contains dataset in zip files which is linked
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ web                     
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ all_data                - will have the data with images and mockups when unziped
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ eval_set                - will have 250 pairs of images and mockups for evaluation
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ training_set            - will have 1500 pairs of images and mockups
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ training_features       - will have 1500 pairs of preprocessed images converted to numpy arrays
‚îú‚îÄ‚îÄ evaluate                - evaluation of model based on BLEU scores
‚îú‚îÄ‚îÄ generated-outputs       - contains the ouputs generated by the model
‚îú‚îÄ‚îÄ logs                    - logs of models
‚îú‚îÄ‚îÄ model                   - this is main working directory of the project 
|¬†¬† ‚îú‚îÄ‚îÄ classes
|   |¬†¬† ‚îú‚îÄ‚îÄ dataset                 - contains datasets generators
|   ‚îî‚îÄ‚îÄ¬†‚îî‚îÄ‚îÄ model                   - contains the implementation of AI models
‚îî‚îÄ‚îÄ model-architectures     - contains viualization of model architecture and thier summaries 

```

## Usage

> Prepare the Dataset

```
# unzip the data
cd datasets
zip -F pix2code_datasets.zip --out datasets.zip
unzip datasets.zip

cd ../model

# split training set and evaluation set 
# usage: build_datasets.py <input path> 
./build_datasets.py ../datasets/web/all_data

# transform images into numpy arrays in training set (normalized pixel values and resized pictures to smaller files if you need to upload the set to train your model in the cloud)
# usage: convert_imgs_to_arrays.py <input path> <output path>
./convert_imgs_to_arrays.py ../datasets/web/training_set ../datasets/web/training_features
```

The dataset is splitted into two sets:

- Training :: 1500 pair of image and markups placed in ```datasets/train/```
- Evaluation :: 250 pair of image and markups placed in ```datasets/eval/```

> Model Training

```
cd model

# provide input path to training data and output path to save trained model and metadata
# usage: train.py <input path> <output path> <train_autoencoder>
./train.py ../datasets/web/training_set ../bin

# train on images pre-processed as converted to numpy arrays
./train.py ../datasets/web/training_features ../bin

# train with autoencoder
./train.py ../datasets/web/training_features ../bin 1
```

> Generate Code for an Image

```
mkdir generated-output
cd model

# generate DSL code aka the .gui file, the default search method is greedy
# usage: sample.py <trained weights path> <trained model name> <input image> <output path> <search method (default: greedy)>
./sample.py ../bin pix2code2 ../test_gui.png ../generated-output

# equivalent to command above
./sample.py ../bin pix2code2 ../test_gui.png ../code greedy

# generation with beam search is coming soon
```

> Compile the .gui code to HTML

```
cd compiler

# compile .gui file to HTML (Bootstrap style)
# usage: web-compiler.py <input file path>.gui
./web-compiler.py ../generated-output/dot_gui.file
```

### Aknowledgement

- This project done in on top of the original work by Tony Beltramelli's [Paper](https://arxiv.org/pdf/1705.07962.pdf) and the [Datasets](https://github.com/tonybeltramelli/pix2code/tree/master/datasets) from him.

- [1] for more information about the AutoEncoders, refer to chapter 14 of the [Deep Learning Book](https://www.deeplearningbook.org/contents/autoencoders.html) by Ian Goodfellow, Yoshua Bengio and Aaron Courville.

- Other references coming soon üëç
